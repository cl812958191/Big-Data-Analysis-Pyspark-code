{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import sys\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "from pyspark import SparkContext\n",
    "from operator import add\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freqArray (listOfIndices, numberofwords):\n",
    "    returnVal = np.zeros (20000)\n",
    "    for index in listOfIndices:\n",
    "        returnVal[index] = returnVal[index] + 1\n",
    "    returnVal = np.divide(returnVal, numberofwords)\n",
    "    return returnVal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "smallTrainFile='SmallTrainingData.txt'\n",
    "TestFile='TestingData.txt'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('AU466', array([2.95839346e-05, 1.81015855e-05, 0.00000000e+00, ...,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00])), ('AU1356', array([2.41667689e-05, 1.28667866e-05, 0.00000000e+00, ...,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00])), ('19264299', array([2.21781209e-05, 1.09011103e-05, 0.00000000e+00, ...,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00]))]\n",
      "[('11847958', array([8.01289015e-06, 2.46335652e-06, 1.12084154e-05, ...,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00])), ('12503251', array([5.36276535e-06, 2.10156814e-06, 1.69597455e-05, ...,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00])), ('25582326', array([1.00753605e-05, 3.19586232e-06, 1.07265296e-05, ...,\n",
      "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00]))]\n",
      "application in AU: 0.7972972972972973\n",
      "and in AU: 1.0\n",
      "attack in AU: 0.06756756756756757\n",
      "protein in AU: 0.0\n",
      "court in AU: 0.9594594594594594\n",
      "application in Wiki: 0.0056413301662707836\n",
      "and in WIki: 1.0\n",
      "attack in Wiki: 0.12232779097387174\n",
      "protein in Wiki: 0.013064133016627079\n",
      "court in Wiki: 0.16656769596199525\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sc = SparkContext(appName=\"LogisticRegressionTask1\")\n",
    "\n",
    "d_corpus = sc.textFile(smallTrainFile)\n",
    "\n",
    "numberOfDocs =d_corpus.count()\n",
    "\n",
    "d_keyAndText = d_corpus.map(lambda x : (x[x.index('id=\"') + 4 : x.index('\" url=')], x[x.index('\">') + 2:][:-6]))\n",
    "regex = re.compile('[^a-zA-Z]')\n",
    "d_keyAndListOfWords = d_keyAndText.map(lambda x : (str(x[0]), regex.sub(' ', x[1]).lower().split()))\n",
    "\n",
    "\n",
    "#Now get the top 20,000 words... first change (docID, [\"word1\", \"word2\", \"word3\", ...])\n",
    "# to (\"word1\", 1) (\"word2\", 1)...\n",
    "allWords = d_keyAndListOfWords.flatMap(lambda x: ((i,1) for i in x[1]))\n",
    "\n",
    "\n",
    "# Now, count all of the words, giving us (\"word1\", 1433), (\"word2\", 3423423), etc.\n",
    "allCounts = allWords.reduceByKey(lambda x,y: x+y)\n",
    "\n",
    "# Get the top 20,000 words in a local array in a sorted format based on frequency\n",
    "# If you want to run it on your laptio, it may a longer time for top 20k words. \n",
    "topWords = allCounts.top(20000, lambda x : x[1])\n",
    "# We'll create a RDD that has a set of (word, dictNum) pairs\n",
    "# start by creating an RDD that has the number 0 through 20000\n",
    "# 20000 is the number of words that will be in our dictionary\n",
    "\n",
    "topWordsK = sc.parallelize(range(20000))\n",
    "\n",
    "# Now, we transform (0), (1), (2), ... to (\"MostCommonWord\", 0)\n",
    "# (\"NextMostCommon\", 1), ...\n",
    "# the number will be the spot in the dictionary used to tell us\n",
    "# where the word is located\n",
    "dictionary = topWordsK.map (lambda x : (topWords[x][0], x))\n",
    "\n",
    "# Next, we get a RDD that has, for each (docID, [\"word1\", \"word2\", \"word3\", ...]),\n",
    "# (\"word1\", docID), (\"word2\", docId), ...\n",
    "\n",
    "allWordsWithDocID = d_keyAndListOfWords.flatMap(lambda x: ((j, x[0]) for j in x[1]))\n",
    "\n",
    "# Now join and link them, to get a set of (\"word1\", (dictionaryPos, docID)) pairs\n",
    "allDictionaryWords = dictionary.join (allWordsWithDocID)\n",
    "\n",
    "\n",
    "# Now, we drop the actual word itself to get a set of (docID, dictionaryPos) pairs\n",
    "justDocAndPos = allDictionaryWords.map (lambda x: (x[1][1], x[1][0]))\n",
    "\n",
    "\n",
    "# Now get a set of (docID, [dictionaryPos1, dictionaryPos2, dictionaryPos3...]) pairs\n",
    "allDictionaryWordsInEachDoc = justDocAndPos.groupByKey()\n",
    "\n",
    "\n",
    "# The following line this gets us a set of\n",
    "# (docID,  [dictionaryPos1, dictionaryPos2, dictionaryPos3...]) pairs\n",
    "# and converts the dictionary positions to a bag-of-words numpy array...\n",
    "allDocsAsNumpyArrays = allDictionaryWordsInEachDoc.map(lambda x: (x[0], freqArray(x[1],len(x[1]))))\n",
    "\n",
    "\n",
    "zeroOrOne = allDocsAsNumpyArrays.map(lambda x: (x[0], np.clip(np.multiply(x[1], 9e9), 0, 1)))\n",
    "# Now, add up all of those arrays into a single array, where the\n",
    "# i^th entry tells us how many\n",
    "# individual documents the i^th word in the dictionary appeared in\n",
    "dfArray = zeroOrOne.reduce(lambda x1, x2: (\"\", np.add(x1[1], x2[1])))[1]\n",
    "\n",
    "# Create an array of 20,000 entries, each entry with the value numberOfDocs (number of docs)\n",
    "multiplier = np.full(20000, numberOfDocs)\n",
    "\n",
    "# Get the version of dfArray where the i^th entry is the inverse-document frequency for the\n",
    "# i^th word in the corpus\n",
    "idfArray = np.log(np.divide(np.full(20000, numberOfDocs), dfArray))\n",
    "\n",
    "# Finally, convert all of the tf vectors in allDocsAsNumpyArrays to tf * idf vectors\n",
    "allDocsAsNumpyArraysTFidf = allDocsAsNumpyArrays.map(lambda x: (x[0], np.multiply(x[1], idfArray)))\n",
    "\n",
    "print(allDocsAsNumpyArraysTFidf.take(3))\n",
    "\n",
    "\n",
    "# test set\n",
    "test_corpus = sc.textFile(TestFile)\n",
    "\n",
    "numberOfDocs =test_corpus.count()\n",
    "\n",
    "test_keyAndText = test_corpus.map(lambda x : (x[x.index('id=\"') + 4 : x.index('\" url=')], x[x.index('\">') + 2:][:-6]))\n",
    "regex = re.compile('[^a-zA-Z]')\n",
    "test_keyAndListOfWords = test_keyAndText.map(lambda x : (str(x[0]), regex.sub(' ', x[1]).lower().split()))\n",
    "\n",
    "t_allWordsWithDocID = test_keyAndListOfWords.flatMap(lambda x: ((j, x[0]) for j in x[1]))\n",
    "\n",
    "t_allDictionaryWords = dictionary.join (t_allWordsWithDocID)\n",
    "\n",
    "t_justDocAndPos = t_allDictionaryWords.map (lambda x: (x[1][1], x[1][0]))\n",
    "\n",
    "t_allDictionaryWordsInEachDoc = t_justDocAndPos.groupByKey()\n",
    "\n",
    "\n",
    "t_allDocsAsNumpyArrays = t_allDictionaryWordsInEachDoc.map(lambda x: (x[0], freqArray(x[1],len(x[1]))))\n",
    "\n",
    "t_zeroOrOne = t_allDocsAsNumpyArrays.map(lambda x: (x[0], np.clip (np.multiply (x[1], 9e9), 0, 1)))\n",
    "t_dfArray = t_zeroOrOne.reduce(lambda x1, x2: (\"\", np.add(x1[1], x2[1])))[1]\n",
    "t_idfArray = np.log(np.divide(np.full(20000, numberOfDocs), t_dfArray))\n",
    "t_allDocsAsNumpyArraysTFidf = t_allDocsAsNumpyArrays.map(lambda x: (x[0], np.multiply(x[1], t_idfArray)))\n",
    "\n",
    "print(t_allDocsAsNumpyArraysTFidf.take(3))\n",
    "\n",
    "\n",
    "application=dictionary.filter(lambda x: x[0]=='applicant').collect()[0][1]\n",
    "and_=dictionary.filter(lambda x: x[0]=='and').collect()[0][1]\n",
    "attack=dictionary.filter(lambda x: x[0]=='attack').collect()[0][1]\n",
    "protein=dictionary.filter(lambda x: x[0]=='protein').collect()[0][1]\n",
    "court=dictionary.filter(lambda x: x[0]=='court').collect()[0][1]\n",
    "\n",
    "zero_or_one1=zeroOrOne.map(lambda x:(1 if 'AU' in x[0] else 0, x[1]))\n",
    "zero_or_one2=zero_or_one1.filter(lambda x: x[0]==1)\n",
    "sum1=zero_or_one2.reduce(lambda x1,x2:('',np.add(x1[1],x2[1])))[1]\n",
    "zero_or_one3=zero_or_one1.filter(lambda x: x[0]==0)\n",
    "sum2=zero_or_one3.reduce(lambda x1,x2:('',np.add(x1[1],x2[1])))[1]\n",
    "\n",
    "print('application in AU:',sum1[application]/zero_or_one2.count())\n",
    "print('and in AU:',sum1[and_]/zero_or_one2.count())\n",
    "print('attack in AU:',sum1[attack]/zero_or_one2.count())\n",
    "print('protein in AU:',sum1[protein]/zero_or_one2.count())\n",
    "print('court in AU:',sum1[court]/zero_or_one2.count())\n",
    "\n",
    "\n",
    "print('application in Wiki:',sum2[application]/zero_or_one3.count())\n",
    "print('and in WIki:',sum2[and_]/zero_or_one3.count())\n",
    "print('attack in Wiki:',sum2[attack]/zero_or_one3.count())\n",
    "print('protein in Wiki:',sum2[protein]/zero_or_one3.count())\n",
    "print('court in Wiki:',sum2[court]/zero_or_one3.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_double(list):\n",
    "    #list[0].astype(int)\n",
    "    list[0].astype('double')\n",
    "    list[176].astype('double')\n",
    "    return list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('AU466',\n",
       "  array([0.10181311, 0.06229661, 0.0125523 , ..., 0.        , 0.        ,\n",
       "         0.        ]))]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allDocsAsNumpyArraysDouble=allDocsAsNumpyArrays.map(lambda x: (x[0],set_double(x[1])))\n",
    "allDocsAsNumpyArraysDouble.take(1)\n",
    "# Now, create a version of allDocsAsNumpyArrays where, in the array,\n",
    "# every entry is either zero or one.\n",
    "# A zero means that the word does not occur,\n",
    "# and a one means that it does.\n",
    "zeroOrOne = allDocsAsNumpyArraysDouble.map(lambda x: (x[0], np.clip (np.multiply (x[1], 9e9), 0, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
